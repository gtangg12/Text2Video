{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ggJ2EXEP4t7Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "class FacePredict(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize using a pretrained tf model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(28, 60)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        self.dense = nn.Linear(60, 20)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        hid0, _ = self.lstm(inputs)\n",
    "        #hiddrop = self.dropout(hid0)\n",
    "        return self.dense(hid0)\n",
    "    \n",
    "    def load_weights_tf(self):\n",
    "        #get the weights from tf model\n",
    "        with torch.no_grad():\n",
    "            #reorder weights to convert from tf to torch\n",
    "            wii, wic, wif, wio = np.split(weights[2][:28, :], 4, 1)\n",
    "            whi, whc, whf, who = np.split(weights[2][28:, :], 4, 1)\n",
    "            wih = np.concatenate((wii, wif, wic, wio), axis = 1)\n",
    "            whh = np.concatenate((whi, whf, whc, who), axis = 1)\n",
    "\n",
    "            self.lstm.weight_ih_l0.data = torch.from_numpy(wih).transpose(0,1)\n",
    "            self.lstm.weight_hh_l0.data = torch.from_numpy(whh).transpose(0,1)\n",
    "            self.lstm.bias_hh_l0.data = torch.from_numpy(weights[3])\n",
    "            self.lstm.bias_ih_l0.data = torch.zeros((240))\n",
    "\n",
    "            self.dense.weight.data = torch.from_numpy(weights[0].T)\n",
    "            self.dense.bias.data = torch.from_numpy(weights[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pAN2xhGC4t7S"
   },
   "outputs": [],
   "source": [
    "def get_audio_derivatives(audio):\n",
    "    #calculate audio derivatives, return timestamps too\n",
    "    audiodiff = audio[1:,:-1] - audio[:-1, :-1]\n",
    "    times = audio[:, -1]\n",
    "    return np.concatenate((audio[:-1, :-1], audiodiff[:, :]), axis=1), times\n",
    "\n",
    "def shifted_time(i, times):\n",
    "      if i >= 20:\n",
    "        return times[i - 20]\n",
    "      else:\n",
    "        return times[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DK-tgFDI4t7V"
   },
   "outputs": [],
   "source": [
    "class FacePredictFineTune(FacePredict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nn.init.xavier_uniform_(self.dense.weight)\n",
    "        nn.init.zeros_(self.dense.bias)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(28) #batch normalization on inputs\n",
    "    def forward(self, inputs):\n",
    "        #shape T*B*D (time*batch*num_feat)\n",
    "        inputs_norm = self.bn(inputs.transpose(1,2)).transpose(1,2)\n",
    "        hid0, _ = self.lstm(inputs_norm)\n",
    "        #hiddrop = self.dropout(hid0)\n",
    "        return self.dense(hid0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tj_1jBQX7xOF",
    "outputId": "0a7f75f8-daf2-4a05-9bdd-64da2c92e1d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpqUXq-c4t7X",
    "outputId": "a2e55f10-f73e-4710-c4ee-57ac2fa74689"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FacePredictFineTune(\n",
       "  (lstm): LSTM(28, 60)\n",
       "  (dense): Linear(in_features=60, out_features=20, bias=True)\n",
       "  (bn): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FacePredictFineTune()\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/6869/face predict'))\n",
    "model.double()\n",
    "#torch.save(fpf.state_dict(), 'face predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpELyNiV4t7Z",
    "outputId": "cc222685-a801-4fcd-9f61-11a5c8d28ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57190, 28)\n"
     ]
    }
   ],
   "source": [
    "audio_preprocessed = np.load('/content/drive/MyDrive/6869/xAAmF3H0-ek_audio.npy')\n",
    "audio_data = get_audio_derivatives(audio_preprocessed)[0]\n",
    "print(audio_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LonmBhL98bRL",
    "outputId": "6cad07c0-04a3-4bff-fb77-44a926dc6117"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16947, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data = np.load('/content/drive/MyDrive/6869/xAAmF3H0-ek_landmarks_frontalized.npy').reshape(-1, 25, 2)\n",
    "video_lip_fiducials = video_data[:, 5:].reshape(-1, 40)\n",
    "video_lip_fiducials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85tKk-fX8cEG",
    "outputId": "0e3b5346-87ce-4810-cc9b-7432f119dcff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crop and shift of video, in seconds\n",
    "video_start = 12*100//30\n",
    "video_end = 16958*100//30 #inclusive\n",
    "video_shft = 200\n",
    "video_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u9hFpph8_UqY"
   },
   "outputs": [],
   "source": [
    "#preprocess video using pca\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 20)\n",
    "lip_features = pca.fit_transform(video_lip_fiducials)\n",
    "\n",
    "#upsampling\n",
    "from scipy.interpolate import interp1d\n",
    "video_times = np.arange(12, 16959)/30\n",
    "lips_interpolate = interp1d(video_times, lip_features, axis = 0)\n",
    "audio_times = np.arange(video_start, video_end)/100\n",
    "lips_upsampled = lips_interpolate(audio_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8ZUVh2XO4t7Y"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "class FacePredictDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs, predict_delay, output_begin, num_cuts = 18):\n",
    "        #temporally inputs[output_begin] matches with outputs[0]\n",
    "        #in rnn match inputs[output_begin + predict_delay] with outputs[0] \n",
    "\n",
    "        #crop outputs\n",
    "        output_length = len(outputs)\n",
    "        crop_len = output_length // num_cuts\n",
    "        self.outputs = [outputs[crop_len*n:crop_len*(n+1)] for n in range(num_cuts)]\n",
    "\n",
    "        #find matching parts of inputs\n",
    "        self.inputs = [inputs[crop_len*n + output_begin: crop_len*(n+1) + output_begin + predict_delay] for n in range(num_cuts)]\n",
    "        self.len = num_cuts\n",
    "        self.crop_len = crop_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.inputs[idx], self.outputs[idx], self.crop_len * idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DF5ftbo9HXIJ"
   },
   "outputs": [],
   "source": [
    "data = FacePredictDataset(audio_data, lips_upsampled, video_shft, video_start, 18)\n",
    "train_data, test_data = torch.utils.data.random_split(data, [16, 2])\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ImCq81_KJcHh"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def delay_loss(preds, y, loss, delay):\n",
    "    return loss(preds[:, delay:, :], y)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optim = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QhXuio8HhRA",
    "outputId": "1587db65-2c6e-44a8-ae4d-33d38e45b317"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/260 [00:12<02:28,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  19\n",
      "training loss: tensor(8.4759, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(7.9280, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 40/260 [00:24<02:15,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  39\n",
      "training loss: tensor(8.0662, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(7.5390, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 60/260 [00:37<02:08,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  59\n",
      "training loss: tensor(7.8522, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(7.2958, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 80/260 [00:49<01:51,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  79\n",
      "training loss: tensor(7.6935, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.9541, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 100/260 [01:01<01:39,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  99\n",
      "training loss: tensor(7.5195, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.9716, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 120/260 [01:14<01:29,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  119\n",
      "training loss: tensor(7.4042, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.8356, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 140/260 [01:26<01:15,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  139\n",
      "training loss: tensor(7.2061, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.5243, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 160/260 [01:39<01:02,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  159\n",
      "training loss: tensor(7.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.5680, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 180/260 [01:51<00:50,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  179\n",
      "training loss: tensor(6.9889, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.4661, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 200/260 [02:03<00:37,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  199\n",
      "training loss: tensor(7.0152, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.2190, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 220/260 [02:16<00:25,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  219\n",
      "training loss: tensor(6.9694, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.2009, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 240/260 [02:28<00:12,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  239\n",
      "training loss: tensor(6.9211, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.1688, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [02:41<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  259\n",
      "training loss: tensor(6.9410, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "validation loss: tensor(6.2837, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in tqdm(range(260)):\n",
    "    for X, y, _ in train_dataloader:\n",
    "        preds = model(X.double())\n",
    "        l = delay_loss(preds, y, loss, video_shft)\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    if epoch % 20 == 19:\n",
    "        print('epoch: ', epoch)\n",
    "        print('training loss:', l)\n",
    "        for X_val, y_val, _ in test_dataloader:\n",
    "            preds_val = model(X_val.double())\n",
    "            l_val = delay_loss(preds_val, y_val, loss, video_shft)\n",
    "            print('validation loss:', l_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kt2Mj_DWI_-k",
    "outputId": "beedda71-d751-4bc4-cb71-22abc6e0d67e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25104)\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val, val_starts = test_dataloader.__iter__().next()\n",
    "preds_val = model(X_val.double())\n",
    "sample_lips = pca.inverse_transform(preds_val[0].detach().numpy())\n",
    "(print(val_starts[0])*30/100) + 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PlNeCSSdO9MM"
   },
   "outputs": [],
   "source": [
    "np.save('/content/drive/MyDrive/6869/sample_lips', sample_lips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCbcwrkbQ5fN",
    "outputId": "f9d683a7-2e1c-4a72-83c8-4386bfe1499e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7543.2"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25104*30/100 + 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03ztjBP8RTke",
    "outputId": "f07cd851-e5bc-45f2-c527-42d0679d7d93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941.4"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-3WPBr0Rgpr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
