{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('ml': conda)",
      "language": "python",
      "name": "python388jvsc74a57bd06d55b71dc135b52e82d5f98f8e1794e021dfdf681f9370d588ff2fb4ac316953"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "RNN Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggJ2EXEP4t7Q"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import os\n",
        "class FacePredict(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize using a pretrained tf model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(28, 60)\n",
        "        #self.dropout = nn.Dropout(p=0.5)\n",
        "        self.dense = nn.Linear(60, 20)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        hid0, _ = self.lstm(inputs)\n",
        "        #hiddrop = self.dropout(hid0)\n",
        "        return self.dense(hid0)\n",
        "    \n",
        "    def load_weights_tf(self):\n",
        "        #get the weights from tf model\n",
        "        with torch.no_grad():\n",
        "            #reorder weights to convert from tf to torch\n",
        "            wii, wic, wif, wio = np.split(weights[2][:28, :], 4, 1)\n",
        "            whi, whc, whf, who = np.split(weights[2][28:, :], 4, 1)\n",
        "            wih = np.concatenate((wii, wif, wic, wio), axis = 1)\n",
        "            whh = np.concatenate((whi, whf, whc, who), axis = 1)\n",
        "\n",
        "            self.lstm.weight_ih_l0.data = torch.from_numpy(wih).transpose(0,1)\n",
        "            self.lstm.weight_hh_l0.data = torch.from_numpy(whh).transpose(0,1)\n",
        "            self.lstm.bias_hh_l0.data = torch.from_numpy(weights[3])\n",
        "            self.lstm.bias_ih_l0.data = torch.zeros((240))\n",
        "\n",
        "            self.dense.weight.data = torch.from_numpy(weights[0].T)\n",
        "            self.dense.bias.data = torch.from_numpy(weights[1])\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAN2xhGC4t7S"
      },
      "source": [
        "def get_audio_derivatives(audio):\n",
        "    #calculate audio derivatives, return timestamps too\n",
        "    audiodiff = audio[1:,:-1] - audio[:-1, :-1]\n",
        "    times = audio[:, -1]\n",
        "    return np.concatenate((audio[:-1, :-1], audiodiff[:, :]), axis=1), times\n",
        "\n",
        "def shifted_time(i, times):\n",
        "      if i >= 20:\n",
        "        return times[i - 20]\n",
        "      else:\n",
        "        return times[0]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK-tgFDI4t7V"
      },
      "source": [
        "class FacePredictFineTune(FacePredict):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        nn.init.xavier_uniform_(self.dense.weight)\n",
        "        nn.init.zeros_(self.dense.bias)\n",
        "        \n",
        "        self.bn = nn.BatchNorm1d(28) #batch normalization on inputs\n",
        "    def forward(self, inputs):\n",
        "        #shape T*B*D (time*batch*num_feat)\n",
        "        inputs_norm = self.bn(inputs.transpose(1,2)).transpose(1,2)\n",
        "        hid0, _ = self.lstm(inputs_norm)\n",
        "        #hiddrop = self.dropout(hid0)\n",
        "        return self.dense(hid0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj_1jBQX7xOF",
        "outputId": "0a7f75f8-daf2-4a05-9bdd-64da2c92e1d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpqUXq-c4t7X",
        "outputId": "a2e55f10-f73e-4710-c4ee-57ac2fa74689"
      },
      "source": [
        "model = FacePredictFineTune()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/6869/face predict'))\n",
        "model.double()\n",
        "#torch.save(fpf.state_dict(), 'face predict')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FacePredictFineTune(\n",
              "  (lstm): LSTM(28, 60)\n",
              "  (dense): Linear(in_features=60, out_features=20, bias=True)\n",
              "  (bn): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpELyNiV4t7Z",
        "outputId": "cc222685-a801-4fcd-9f61-11a5c8d28ce9"
      },
      "source": [
        "audio_preprocessed = np.load('/content/drive/MyDrive/6869/xAAmF3H0-ek_audio.npy')\n",
        "audio_data = get_audio_derivatives(audio_preprocessed)[0]\n",
        "print(audio_data.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(57190, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LonmBhL98bRL",
        "outputId": "6cad07c0-04a3-4bff-fb77-44a926dc6117"
      },
      "source": [
        "video_data = np.load('/content/drive/MyDrive/6869/xAAmF3H0-ek_landmarks_frontalized.npy').reshape(-1, 25, 2)\n",
        "video_lip_fiducials = video_data[:, 5:].reshape(-1, 40)\n",
        "video_lip_fiducials.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16947, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85tKk-fX8cEG",
        "outputId": "0e3b5346-87ce-4810-cc9b-7432f119dcff"
      },
      "source": [
        "#crop and shift of video, in seconds\n",
        "video_start = 12*100//30\n",
        "video_end = 16958*100//30 #inclusive\n",
        "video_shft = 200\n",
        "video_start"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9hFpph8_UqY"
      },
      "source": [
        "#preprocess video using pca\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 20)\n",
        "lip_features = pca.fit_transform(video_lip_fiducials)\n",
        "\n",
        "#upsampling\n",
        "from scipy.interpolate import interp1d\n",
        "video_times = np.arange(12, 16959)/30\n",
        "lips_interpolate = interp1d(video_times, lip_features, axis = 0)\n",
        "audio_times = np.arange(video_start, video_end)/100\n",
        "lips_upsampled = lips_interpolate(audio_times)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZUVh2XO4t7Y"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "class FacePredictDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs, predict_delay, output_begin, num_cuts = 18):\n",
        "        #temporally inputs[output_begin] matches with outputs[0]\n",
        "        #in rnn match inputs[output_begin + predict_delay] with outputs[0] \n",
        "\n",
        "        #crop outputs\n",
        "        output_length = len(outputs)\n",
        "        crop_len = output_length // num_cuts\n",
        "        self.outputs = [outputs[crop_len*n:crop_len*(n+1)] for n in range(num_cuts)]\n",
        "\n",
        "        #find matching parts of inputs\n",
        "        self.inputs = [inputs[crop_len*n + output_begin: crop_len*(n+1) + output_begin + predict_delay] for n in range(num_cuts)]\n",
        "        self.len = num_cuts\n",
        "        self.crop_len = crop_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return [self.inputs[idx], self.outputs[idx], self.crop_len * idx]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF5ftbo9HXIJ"
      },
      "source": [
        "data = FacePredictDataset(audio_data, lips_upsampled, video_shft, video_start, 18)\n",
        "train_data, test_data = torch.utils.data.random_split(data, [16, 2])\n",
        "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=2, shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCq81_KJcHh"
      },
      "source": [
        "from torch import optim\n",
        "def delay_loss(preds, y, loss, delay):\n",
        "    return loss(preds[:, delay:, :], y)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optim = optim.Adam(model.parameters())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QhXuio8HhRA",
        "outputId": "1587db65-2c6e-44a8-ae4d-33d38e45b317"
      },
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in tqdm(range(260)):\n",
        "    for X, y, _ in train_dataloader:\n",
        "        preds = model(X.double())\n",
        "        l = delay_loss(preds, y, loss, video_shft)\n",
        "        l.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    if epoch % 20 == 19:\n",
        "        print('epoch: ', epoch)\n",
        "        print('training loss:', l)\n",
        "        for X_val, y_val, _ in test_dataloader:\n",
        "            preds_val = model(X_val.double())\n",
        "            l_val = delay_loss(preds_val, y_val, loss, video_shft)\n",
        "            print('validation loss:', l_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 20/260 [00:12<02:28,  1.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  19\n",
            "training loss: tensor(8.4759, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(7.9280, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 40/260 [00:24<02:15,  1.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  39\n",
            "training loss: tensor(8.0662, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(7.5390, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 60/260 [00:37<02:08,  1.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  59\n",
            "training loss: tensor(7.8522, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(7.2958, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 31%|███       | 80/260 [00:49<01:51,  1.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  79\n",
            "training loss: tensor(7.6935, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.9541, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 100/260 [01:01<01:39,  1.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  99\n",
            "training loss: tensor(7.5195, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.9716, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 120/260 [01:14<01:29,  1.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  119\n",
            "training loss: tensor(7.4042, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.8356, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 140/260 [01:26<01:15,  1.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  139\n",
            "training loss: tensor(7.2061, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.5243, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 160/260 [01:39<01:02,  1.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  159\n",
            "training loss: tensor(7.1122, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.5680, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 180/260 [01:51<00:50,  1.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  179\n",
            "training loss: tensor(6.9889, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.4661, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 200/260 [02:03<00:37,  1.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  199\n",
            "training loss: tensor(7.0152, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.2190, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%|████████▍ | 220/260 [02:16<00:25,  1.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  219\n",
            "training loss: tensor(6.9694, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.2009, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 240/260 [02:28<00:12,  1.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  239\n",
            "training loss: tensor(6.9211, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.1688, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 260/260 [02:41<00:00,  1.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  259\n",
            "training loss: tensor(6.9410, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
            "validation loss: tensor(6.2837, dtype=torch.float64, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt2Mj_DWI_-k",
        "outputId": "beedda71-d751-4bc4-cb71-22abc6e0d67e"
      },
      "source": [
        "X_val, y_val, val_starts = test_dataloader.__iter__().next()\n",
        "preds_val = model(X_val.double())\n",
        "sample_lips = pca.inverse_transform(preds_val[0].detach().numpy())\n",
        "(print(val_starts[0])*30/100) + 12"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(25104)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlNeCSSdO9MM"
      },
      "source": [
        "np.save('/content/drive/MyDrive/6869/sample_lips', sample_lips)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCbcwrkbQ5fN",
        "outputId": "f9d683a7-2e1c-4a72-83c8-4386bfe1499e"
      },
      "source": [
        "25104*30/100 + 12"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7543.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03ztjBP8RTke",
        "outputId": "f07cd851-e5bc-45f2-c527-42d0679d7d93"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "941.4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-3WPBr0Rgpr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}