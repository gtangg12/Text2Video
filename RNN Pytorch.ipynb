{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from synthesizing_obama_network_training/obama/obama/model.ckpt-300\n",
      "[<tf.Variable 'rnnlm/output_w:0' shape=(60, 20) dtype=float32_ref>, <tf.Variable 'rnnlm/output_b:0' shape=(20,) dtype=float32_ref>, <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(88, 240) dtype=float32_ref>, <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/lstm_cell/bias:0' shape=(240,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#load pretrained weights\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "path = \"synthesizing_obama_network_training/obama/obama/\"\n",
    "saver = tf.compat.v1.train.import_meta_graph(\"synthesizing_obama_network_training/obama/obama/model.ckpt-300.meta\")\n",
    "ckpt = tf.train.get_checkpoint_state(path)\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "variables = tf.compat.v1.trainable_variables()\n",
    "weights = []\n",
    "for var in variables:\n",
    "    weights.append(sess.run(var))\n",
    "print(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle\n",
    "with open(\"./synthesizing_obama_network_training/data/training_obama.cpkl\", \"rb\") as f:\n",
    "    data = _pickle.load(f, encoding = 'latin1')\n",
    "#get mean and std of train data for batch normalization\n",
    "meani, stdi, meano, stdo = data[\"inputmean\"], data[\"inputstd\"], data[\"outputmean\"], data[\"outputstd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "class FacePredict(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize using a pretrained tf model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(28, 60)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        self.dense = nn.Linear(60, 20)\n",
    "        \n",
    "        #get the weights from tf model\n",
    "        with torch.no_grad():\n",
    "            #reorder weights to convert from tf to torch\n",
    "            wii, wic, wif, wio = np.split(weights[2][:28, :], 4, 1)\n",
    "            whi, whc, whf, who = np.split(weights[2][28:, :], 4, 1)\n",
    "            wih = np.concatenate((wii, wif, wic, wio), axis = 1)\n",
    "            whh = np.concatenate((whi, whf, whc, who), axis = 1)\n",
    "\n",
    "            self.lstm.weight_ih_l0.data = torch.from_numpy(wih).transpose(0,1)\n",
    "            self.lstm.weight_hh_l0.data = torch.from_numpy(whh).transpose(0,1)\n",
    "            self.lstm.bias_hh_l0.data = torch.from_numpy(weights[3])\n",
    "            self.lstm.bias_ih_l0.data = torch.zeros((240))\n",
    "\n",
    "            self.dense.weight.data = torch.from_numpy(weights[0].T)\n",
    "            self.dense.bias.data = torch.from_numpy(weights[1])\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        hid0, _ = self.lstm(inputs)\n",
    "        #hiddrop = self.dropout(hid0)\n",
    "        return self.dense(hid0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_derivatives(audio):\n",
    "    #calculate audio derivatives, return timestamps too\n",
    "    audiodiff = audio[1:,:-1] - audio[:-1, :-1]\n",
    "    times = audio[:, -1]\n",
    "    return np.concatenate((audio[:-1, :-1], audiodiff[:, :]), axis=1), times\n",
    "\n",
    "def shifted_time(i, times):\n",
    "      if i >= 20:\n",
    "        return times[i - 20]\n",
    "      else:\n",
    "        return times[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test our weights on obama\n",
    "test_audio = np.load(\"./synthesizing_obama_network_training/obama_data/audio/normalized-cep13/test_audio_preprocessed.wav.npy\")\n",
    "inputs, times = get_audio_derivatives(test_audio)\n",
    "inputs = (inputs - meani) / stdi\n",
    "inputs = torch.Tensor(inputs).unsqueeze(1)\n",
    "\n",
    "fp = FacePredict()\n",
    "outputs = fp(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacePredictFineTune(FacePredict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nn.init.xavier_uniform_(self.dense.weights)\n",
    "        nn.init.zeros_(self.dense.bias)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1D(28) #batch normalization on inputs\n",
    "    def forward(self, inputs):\n",
    "        #shape T*B*D (time*batch*num_feat)\n",
    "        inputs_norm = self.bn(inputs.transpose(1,2)).transpose(1,2)\n",
    "        hid0, _ = self.lstm(inputs_norm)\n",
    "        #hiddrop = self.dropout(hid0)\n",
    "        return self.dense(hid0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "class FacePredictDataset(Dataset):\n",
    "    #post mfcc calculation dataset with audio and landmarks\n",
    "    def __init__(self, video_names, audio_dir, landmarks_dir):\n",
    "        self.video_names = video_names\n",
    "        self.audio_dir = audio_dir\n",
    "        self.landmarks_dir = landmarks_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(self.audio_dir, self.video_names[idx] + '.wav.npy')\n",
    "        audio = np.load(audio_path)\n",
    "        audio_features = get_audio_derivatives(audio)\n",
    "        \n",
    "        landmarks_path = os.path.join(self.landmarks_dir, self.video_names[idx] + '.npy')\n",
    "        landmarks = np.load(landmarks_path)\n",
    "        return {'audio': audio_features, 'landmarks': landmarks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ml': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd06d55b71dc135b52e82d5f98f8e1794e021dfdf681f9370d588ff2fb4ac316953"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
